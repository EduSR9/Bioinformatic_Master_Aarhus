---
title: "Week 02 - linear regression"
output:
  html_document: 
    theme: readable
editor_options: 
  chunk_output_type: inline
---

# Blood pressure

We will work on a small dataset on blood pressure with the following variables:

* Blood pressure (mm Hg)  
* Age (years)  
* Weight (grams)
* Blood pressure class (4 levels)

The Blood pressure class has 4 levels

* Low: <90
* Ideal: 90-120
* Pre-high blood pressure: >120-140
* High bp: >140


```{r}

library(tidyverse)

df <- read_csv(file = "blood_pressure.csv") %>%
  select(-class)

df

```

## Regression with multiple variables

>Q1: Do a full regression model with and without interaction

You should predict blood pressure from age and weight.

How much of the variation is explained by the two models?

Is the interaction significant?
```{r}
# Model with age and weight
model_main <- lm(bp ~ age + weight, data = df)

# Model with interaction between age and weight
model_interaction <- lm(bp ~ age * weight, data = df)

# Result: Adjusted R-squared is 0.9522 (95.22% explained)
summary(model_main)

# Result: Adjusted R-squared is 0.952 (95.20% explained)
summary(model_interaction)

# Test to see if the interaction is significant
# Result: p-value is 0.6121. Since 0.6121 > 0.05, interaction is NOT significant.
anova(model_main, model_interaction)
```


---

>Q2: Do you see anything wrong with the model?

Yep - we're fishing for a residual plot...

```{r}

# Looking at the 'Residuals vs Fitted' plot
# The red line is almost perfectly flat and the points are scattered randomly.
# This means the relationship is linear and I don't need to add squared terms or fancy curves.
plot(model_main, which = 1)

# Looking at the 'Q-Q Residuals' plot
# The points follow the dashed diagonal line really well.
# This confirms the residuals are normally distributed, which is what I want.
plot(model_main, which = 2)

# Looking at 'Scale-Location'
# The spread of the points doesn't change much from left to right.
# This means the variance is constant (homoscedasticity), so the model is stable.
plot(model_main, which = 3)

# Looking at 'Residuals vs Leverage'
# No points are outside the red dashed lines (Cook's distance).
# This means no single outlier (like point 6 or 164) is unfairly pulling the whole model.
plot(model_main, which = 5)

```


---

>Q3: What is the predicted bp of an typical 35 kg 12 year old boy and a 60 year old fat man (250 kg)?

```{r}
# I need to create a small table with the new people I want to predict for.
# Remember checking the units of our original data (I firstly wrote them in kg)
new_people <- data.frame(
  age = c(12, 60), 
  weight = c(35000, 250000)
)

# The prediction with the people
predictions <- predict(model_main, newdata = new_people)

# The first number is the boy, the second is the man.
predictions

# My final notes on why the model is still garbage:
# 1. Even with correct units, the results are medically impossible.
# 2. A BP of 25 means the boy's heart isn't pumping. 
# 3. A BP of 291 means the man's arteries are about to burst.
# 4. Conclusion: The linear relationship simply does not work outside 
#    the narrow age/weight range of the original training data.

```
 

---

# The ALS data

>Q4: What is the estimated performance (RMSE) of a full linear model using all predictors (without interaction)

Hint: repeated cross validation, I suggest 5 runs of 10 fold CV.

Hint 2: lm(y ~ .) means lm(y ~ "all other columns in the dataset).

If you are up for it: also get the training RMSE (by fitting all data and predicting on same data).


```{r, warning=F, message=F}

als_data <- read_rds("../../week_1/exercise/ALS_progression_rate.1822x370.rds")

als_data <- als_data %>% 
  filter(!is.na(dFRS))

head(als_data)

```


```{r}

# Training RMSE

# 1. Fit the full model
model_full_als <- lm(dFRS ~ ., data = als_data)

# 2. Get the residuals (actual dFRS - predicted dFRS)
residuals_als <- residuals(model_full_als)

# 3. Calculate RMSE manually
# Square errors -> Average them -> Take Square Root
rmse_als <- sqrt(mean(residuals_als^2))

# 4. View the result
# Result: 0.4206154
rmse_als

# My comments on the result:
# The RMSE is 0.4206, which is my "typical" prediction error.
# This means that on average, the model is off by 0.42 units of dFRS.
# Since dFRS values in the data are small (mostly between 0 and -3), 
# an error of 0.42 is actually quite largeâ€”it's a significant chunk of the total

# 10-Fold cross validation

# Set seed for reproducibility
set.seed(1)

# Empty vector to store the residuals from the 5 runs
cv_errors <- c()

# For loop for the 5 runs
for (run in 1:5){
  # Adding the tags for the groups randomly for each run
  als_data$tags <- sample(rep(1:10, length.out = nrow(als_data)))
  
  # Inner for loop for the 10 folds
  for (i in 1:10){
    # Train data with only the samples without the tag i
    train_data <- als_data[als_data$tags != i, ]
    
    # The oposite to the train data
    test_data <- als_data[als_data$tags == i, ]
    
    # The linear model using only the train data and removing the tags column
    train_model <- lm(dFRS ~ . - tags, data = train_data)
    
    # Predictions of dFRS for using the test data
    predictions <- predict(train_model, newdata = test_data)
    
    # The calculation of the residuals, the real dFRS - the predicted dFRS
    errors_fold <- test_data$dFRS - predictions
    
    # Adding the errors to the initial vectors
    cv_errors <- c(cv_errors, errors_fold)
  }
}
rmse_folds <- sqrt(mean(cv_errors^2))
rmse_folds

```


# Well done!

