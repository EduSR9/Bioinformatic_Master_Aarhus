---
title: "Statistical & Machine Learning in Bioinformatics"
subtitle: "Week 3: Intro to Bayesian vs Likelihood"
author: "Thomas B."
output:
  html_document:
    theme: paper
    code_folding: show
    toc: true
    toc_depth: 2
editor_options: 
  chunk_output_type: inline
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape2)
```

# Goals for the theoretical exercise & R session this week

-   Start to think about how we use a generative model , aka as
    $F_{\theta}()$ for data, $X$, and how we can infer $\theta$ from
    $X$:

![](visuals/HoHu_GenerativeModel_for_Data.png){width="200"}

credits : Holmes and Huber

-   Get used to data inference with Likelihood and Bayesian analysis,
    test your intuition beyond math formulas

-   Play with the beta distribution often used a as prior distribution
    for proportions

-   Simulate as a way to approximate a (likelihood or) posterior
    distribution

-   Summarize a posterior distribution

Note This session implies that you have read and digested the section on
Bayesian thinking in chapter 2 of the `modern stats for modern biology`
by Holmes & Huber Here is the online textbook material:
<https://www.huber.embl.de/msmb/02-chap.html#bayesian-thinking>

Another good extra read for much more in depth on the beta-binomial
framework: <https://www.bayesrulesbook.com/chapter-3.html>

# The beta-binomial model in a nutshell:

To recap very very briefly the Bayes approach, the three ingredients are
:

The prior on $\theta$ the underlying proportion of the binomial
distribution is Beta distributed

More precisely, the prior distribution $f(\theta)$ on $\theta$: $$
f(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha - 1}(1-\theta)^{\beta - 1}
$$

The likelihood of the data ( $y$ "success" outcomes out of $n$ trials)
under the binomial model is the probability of the data (here $Y= y$)
given the model parameter(s) (here $\theta$):

$$
L(\theta|y) = Prob(data| \theta) = {n \choose y} \theta^{y} (1-\theta)^{n-y}
$$

The posterior $\theta | (Y = y)$ is also Beta distributed ... In
Bayesian jargon when prior and posterior distribution come from the same
"family" we say that they *conjugate*.

$$
\begin{split}
Y \, |\, \theta \ & \sim \text{Bin}(n, \theta) \\
\theta & \sim \text{Beta}(\alpha, \beta) \\
\end{split} \;\; \Rightarrow \;\; 
\theta | (Y = y) \sim \text{Beta}(\alpha + y, \beta + n - y) \; .
$$

# Set up for the exercise

Let's imagine a concrete situation where 300 individuals that tested `+`
for SARS-cov2 are then characterized for the presence of a specific
corona variant (such as the infamous B.1.1.7 aka "variant of public
concern" that started to spread like wildfire in the UK and then Ireland
.. and then ). Out of 300 people tested and sequenced, 40 carry the
variant

## Q1: write a `loglikelihood` R function for the example above

The function of theta that returns for a dataset where $k$ observations
in a given category out of $n$ observations

```{r}
loglikelihood <- function(k, n, teta_vector){
  return(dbinom(k, n, teta_vector, log = T))
}
possible_thetas <- seq(0, 1, by = 0.001)
x <- loglikelihood(40, 300, possible_thetas)
theta_df <- data.frame(theta = possible_thetas, likelihood = x)

ggplot(theta_df, aes(x = theta, y = x)) +
  geom_line() +
  labs(x = "Theta",
       y = "Log-likelihood")
```

------------------------------------------------------------------------

## Beta Priors for the frequency of variants

Here are 3 different possible prior distributions

```{r}
thetas <- seq(0, 1, by = 0.001)
theta <- thetas[1:500]
dfbetas <- tibble(theta,
           db1= dbeta(theta,1,7),
           db2 = dbeta(theta, 5, 35),
           db3 = dbeta(theta, 50, 350))
require(reshape2)
datalong  <-  melt(dfbetas, id="theta")
head(datalong)
require(ggthemes)

ggplot(datalong) +
geom_line(aes(x = theta,y=value,colour=variable), size=1.4) +
theme(legend.title=element_blank()) +
scale_colour_viridis_d(name  ="Priors",
                          labels=c("B(1,7)", "B(5,35)","B(50,350)"))+ 
theme_minimal(base_size = 15)

```

## Which prior would you use for the Bayesian analysis ?

Here you can see that different priors have probability mass in the
0.1-0.2 range for $\theta$ and lets say this is the range of frequencies
one can "a priori" expect given similar data obtained. But clearly some
priors are more peaked than others ... and it is legitimate to wonder
which one should use. In the case of the SARS-cov variant there is the
info brought by previous studies in nearby countries, etc. So for now we
will use the prior that is "intermediate" between very peaked and and
quite flat : Beta(5,35)

Here are many draws in that prior distribution

```{r}
rtheta <- rbeta(n = 100000, shape1 = 5, shape2 = 35)
qplot(rtheta) + 
  theme_minimal()#quick and dirty ggplplot on a vector

```

## Simulating data under the model

Model has an underlying prior for $\theta$ the frequency of a given
variant

And for each model (seeded by a choice of $\theta$ in the prior), we can
generate an observed number of a given variant when 300 individuals are
"observed" or characterized. Below `rtheta` is a vector storing many
draws in a Beta prior `y` is a vector of observed values according to
each $\theta$ chosen in the prior. Note that `rbinom()` is implicitly
vectorized so you can pass as argument to for instance prob a single
number (that will be reused `n` times or a vector):

```{r}
rtheta <- rbeta(n = 10000, shape1 = 5, shape2 = 35)
hist(rtheta) #quick and dirty plot on a vector

y <- rbinom(n = length(rtheta), prob =  rtheta, size = 300)

hist(y, breaks = 50, col = "orange", main = "", xlab = "Yos", probability = T)

```

## Q2: Take break and reflect:

Can you explain to your team mate how are these two vectors

-   `y`

    y represents the number of people that would have that virus variant
    out of 300 in different scenarios where the probability of having
    that variant varies depending on which rtheta are we.

-   `rtheta`

    rtheta are all 10.000 plausible values of frequencies of that
    genetic variant

related to $P(D)$ and $P(\theta)$ in the notes on Bayesian inference and
the celebrated formula :

$$P(\theta|D) = \frac {P(D|\theta) P(\theta)}{P(D)}$$

------------------------------------------------------------------------

## Simulating to approximate the Posterior aka $P(\theta|y)$

You know in this case ( beta conjugate with binomial) that the
prosterior is beta distributed. But lets have some intuition by using
simulations to find the posterior

## Q3: Approximate the posterior distribution of $\theta$

Hint. we have just simulated a large number of datasets under a model
where we first sample the prior for $\theta$ and then based on that
simulate 1 dataset ( a yobs).

Use the (sub) set of simulations (stored in `y`) to approximate the
posterior, "by conditioning on the data" .

```{r}
posterior <- rtheta[y == 40]
(mean(posterior))
```

## Q4: Superimpose / compare with the theoretical posterior

------------------------------------------------------------------------

## Exploiting the posterior

We summarize the posterior distribution by its mean / median for
instance

Check with the direct simulation from the theoretical posterior

```{r}
thetaPostMC <- rbeta(n = 1e6, 45, 295)
mean(thetaPostMC)
```

Or check by using the theoretical mean of a Beta distribution ( google
it!)

### The credible intervals

## Q5: Use the posterior distribution to get credible intervals matching these statements

What is the posterior probability that the variant frequency is:

-   less than 10% ? (we got it moderate wrong)
-   more than 20% (we got it completely wrong)

```{r}
df_posterior <- data.frame(theta = posterior)
ggplot(df_posterior, aes(theta)) + 
  geom_histogram()

less10prob <- length(posterior[posterior < 0.1])/length(posterior) * 100
more20prob <- length(posterior[posterior > 0.2])/length(posterior) * 100

credible_interval <- quantile(posterior, probs = c(0.025, 0.975))

```

------------------------------------------------------------------------

## Exploring sensitivity to the choice of prior

## Q6: Redo the Bayesian analysis replacing our original prior with a softer prior (less peaked), meaning that we use less prior information.

How much does this change the final result?

The use of a softer, less peaked prior (e.g., $Beta(5, 35)$) reflects a
lower level of prior certainty, which allows the observed data to have a
stronger influence on the final result. Consequently, the credible
interval becomes broader and shifts closer to the sample proportion,
representing the increased uncertainty inherent in having fewer prior
"counts".

```{r}
new_rtheta <- rbeta(n = 100000, shape1 = 1, shape2 = 7)

hist(new_rtheta) #quick and dirty plot on a vector

new_y <- rbinom(n = length(new_rtheta), prob =  new_rtheta, size = 300)

hist(new_y, breaks = 50, col = "orange", main = "", xlab = "Yos", probability = T)

new_posterior <- new_rtheta[y == 40]
(mean(new_posterior))

df_new_posterior <- data.frame(theta = new_posterior)
ggplot(df_new_posterior, aes(theta)) + 
  geom_histogram()

less10prob <- length(new_posterior[new_posterior < 0.1])/length(new_posterior) * 100
more20prob <- length(new_posterior[new_posterior > 0.2])/length(new_posterior) * 100

credible_interval <- quantile(new_posterior, probs = c(0.025, 0.975))

```

------------------------------------------------------------------------

## Q7: Go **extreme** and use a flat prior ...

What choice of shape parameters do you need ot make to get a completely
flat prior ?

> Discuss how much weight a flat prior has in the posterior distribution

```{r}
new_rtheta <- rbeta(n = 100000, shape1 = 1, shape2 = 1)

hist(new_rtheta) #quick and dirty plot on a vector

new_y <- rbinom(n = length(new_rtheta), prob =  new_rtheta, size = 300)

hist(new_y, breaks = 50, col = "orange", main = "", xlab = "Yos", probability = T)

new_posterior <- new_rtheta[y == 40]
(mean(new_posterior))

df_new_posterior <- data.frame(theta = new_posterior)
ggplot(df_new_posterior, aes(theta)) + 
  geom_histogram()

less10prob <- length(new_posterior[new_posterior < 0.1])/length(new_posterior) * 100
more20prob <- length(new_posterior[new_posterior > 0.2])/length(new_posterior) * 100

credible_interval <- quantile(new_posterior, probs = c(0.025, 0.975))

```

## Inference using solely the Likelihood principle

Write the likelihood function for the data as a function of $\theta$ and
visualize it as a function of the proportion $\theta$.

Can you also approximate the (rescaled) likelihood by using the
simulations for a given $f(\theta)$ ? Hint if you use a "completely
flat" prior you expect that it will be essentially like a likelihood
analysis. But you can also all $N$ simulations for a given \theta
interval, count how many simulations yielded a given $y_{obs}$
($n_{y_{obs}$) and approximate the likelihood as

$$ Prob(Y=y_{obs}|\theta) \approx \frac{n_{y_{obs}}}{N} $$:

Apply that recipe to approximate the likelihood in intervals of width
$2 \epsilon = 0.05$

Start with for instance Prob(Y=y\_{obs}\|\theta = 0.1)
\approx \frac{n_{y_{obs}}}{N}

```{r}
sims <- tibble(thetas = rtheta,
       yobs = y)

sims %>%
  filter(yobs == 40) %>%
  filter(thetas < 0.11) %>%
  filter(thetas > 0.09) %>%
  tally()

sims %>%
  # filter(yobs == 40) %>%
  filter(thetas < 0.11) %>%
  filter(thetas > 0.09) %>%
  tally()

27/1565

theta_grid <- seq(0.05, 0.25, by = 0.01)
get_likelihood <- function(t_center) {
  epsilon <- 0.025
  
  # Total simulations within the theta interval (N)
  N <- sims %>%
    filter(thetas > (t_center - epsilon) & thetas < (t_center + epsilon)) %>%
    tally() %>% 
    pull(n)
  # Simulations where yobs == 40 within that interval (n_yobs)
  n_yobs <- sims %>%
    filter(yobs == 40) %>%
    filter(thetas > (t_center - epsilon) & thetas < (t_center + epsilon)) %>%
    tally() %>% 
    pull(n)
  
  # Approximate Likelihood: n_yobs / N
  return(n_yobs / N)
}

likelihood_results <- data.frame(
  theta = theta_grid,
  likelihood = sapply(theta_grid, get_likelihood)
)

ggplot(likelihood_results, aes(x = theta, y = likelihood)) +
  geom_line(color = "firebrick", size = 1, alpha = 0.6) +
  geom_point(color = "firebrick") +
  
  stat_function(fun = function(x) dbinom(40, size = 300, prob = x), 
                color = "blue", size = 1, linetype = "dashed") +
  
  labs(title = "Comparison likelihood, simulated vs theoretical binomial",
       subtitle = "Red: Sim (ABC) | Dotted blue: theoretical bin",
       x = expression(theta), y = "L(theta)") +
  theme_minimal()
```

Then you reuse that idea to get the likelihood over a grid of values...

# Comparing Likelihood and Bayesian (if there is time)

## is the mode of a posterior distribution coinciding with the MLE ?

You know what MLE for a binomial is ...

You know (or you google), what is the mode of a Beta distribution (that
your maximum posterior estimate). Do they ever exactly match ?

## Compare the 95% confidence interval with the 95% credible interval

For more on credible intervals:
<https://en.wikipedia.org/wiki/Credible_interval>

You use the likelihood to profile to get a confidence interval for
$\theta$.

------------------------------------------------------------------------
